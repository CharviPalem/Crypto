{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffd93b0",
   "metadata": {},
   "source": [
    "# Evaluation Demo\n",
    "\n",
    "This notebook demonstrates comprehensive evaluation analysis of the FHE NLP project results.\n",
    "\n",
    "## Overview\n",
    "- Load evaluation results from `data/results/evaluation.json`\n",
    "- Visualize performance metrics (accuracy, precision, recall, F1-score)\n",
    "- Plot privacy-utility trade-off curves (Œµ vs accuracy)\n",
    "- Analyze security and privacy metrics\n",
    "- Summarize best configurations and recommendations\n",
    "\n",
    "## Key Features\n",
    "- **Performance Analysis**: Comprehensive ML metrics visualization\n",
    "- **Privacy-Utility Trade-offs**: Differential privacy parameter analysis\n",
    "- **Security Assessment**: FHE security metrics evaluation\n",
    "- **Configuration Optimization**: Best parameter recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16\n",
    "})\n",
    "\n",
    "print(\"üìö All imports successful!\")\n",
    "print(\"üé® Plotting style configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d7960",
   "metadata": {},
   "source": [
    "## 1. Load Evaluation Results\n",
    "\n",
    "Load the comprehensive evaluation results from the pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results\n",
    "evaluation_file = Path('../data/results/evaluation.json')\n",
    "\n",
    "if evaluation_file.exists():\n",
    "    print(f\"üìÇ Loading evaluation results from: {evaluation_file}\")\n",
    "    with open(evaluation_file, 'r', encoding='utf-8') as f:\n",
    "        evaluation_data = json.load(f)\n",
    "    print(\"‚úÖ Evaluation data loaded successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Evaluation file not found. Creating synthetic evaluation data for demonstration...\")\n",
    "    \n",
    "    # Create comprehensive synthetic evaluation data\n",
    "    evaluation_data = {\n",
    "        \"performance_metrics\": {\n",
    "            \"accuracy\": 0.847,\n",
    "            \"precision\": 0.823,\n",
    "            \"recall\": 0.871,\n",
    "            \"f1_score\": 0.846,\n",
    "            \"roc_auc\": 0.912,\n",
    "            \"classification_report\": {\n",
    "                \"diabetes\": {\"precision\": 0.85, \"recall\": 0.82, \"f1-score\": 0.84, \"support\": 45},\n",
    "                \"hypertension\": {\"precision\": 0.81, \"recall\": 0.89, \"f1-score\": 0.85, \"support\": 52},\n",
    "                \"heart_disease\": {\"precision\": 0.79, \"recall\": 0.88, \"f1-score\": 0.83, \"support\": 38},\n",
    "                \"asthma\": {\"precision\": 0.88, \"recall\": 0.85, \"f1-score\": 0.86, \"support\": 41},\n",
    "                \"arthritis\": {\"precision\": 0.82, \"recall\": 0.91, \"f1-score\": 0.86, \"support\": 47}\n",
    "            }\n",
    "        },\n",
    "        \"privacy_metrics\": {\n",
    "            \"k_anonymity\": {\"k_value\": 5, \"privacy_level\": \"high\"},\n",
    "            \"l_diversity\": {\"l_value\": 3, \"privacy_level\": \"medium\"},\n",
    "            \"information_leakage\": {\"leakage_score\": 0.12, \"privacy_level\": \"high\"},\n",
    "            \"differential_privacy\": {\"epsilon\": 1.0, \"privacy_level\": \"medium\"},\n",
    "            \"overall_assessment\": {\"overall_privacy_level\": \"high\", \"privacy_score\": 2.8}\n",
    "        },\n",
    "        \"security_metrics\": {\n",
    "            \"overall_security\": {\"overall_security_score\": 87.3},\n",
    "            \"attack_resistance\": {\"overall_attack_resistance_score\": 84.7},\n",
    "            \"key_security\": {\"key_security_strength\": 128},\n",
    "            \"noise_analysis\": {\"initial_noise_budget_bits\": 412.5}\n",
    "        },\n",
    "        \"privacy_utility_experiments\": {\n",
    "            \"epsilon_values\": [0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "            \"accuracy_values\": [0.721, 0.789, 0.847, 0.863, 0.871],\n",
    "            \"precision_values\": [0.698, 0.765, 0.823, 0.841, 0.849],\n",
    "            \"recall_values\": [0.743, 0.812, 0.871, 0.884, 0.892],\n",
    "            \"f1_values\": [0.720, 0.788, 0.846, 0.862, 0.870]\n",
    "        },\n",
    "        \"model_comparison\": {\n",
    "            \"logistic_regression\": {\n",
    "                \"clear_accuracy\": 0.863,\n",
    "                \"fhe_accuracy\": 0.847,\n",
    "                \"training_time\": 2.34,\n",
    "                \"inference_time\": 0.0023\n",
    "            },\n",
    "            \"svm\": {\n",
    "                \"clear_accuracy\": 0.841,\n",
    "                \"fhe_accuracy\": 0.829,\n",
    "                \"training_time\": 4.67,\n",
    "                \"inference_time\": 0.0156\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    print(\"‚úÖ Synthetic evaluation data created for demonstration\")\n",
    "\n",
    "# Display basic information about the loaded data\n",
    "print(f\"\\nüìä Evaluation Data Overview:\")\n",
    "print(f\"   Available sections: {list(evaluation_data.keys())}\")\n",
    "\n",
    "# Extract key metrics for quick overview\n",
    "if 'performance_metrics' in evaluation_data:\n",
    "    perf = evaluation_data['performance_metrics']\n",
    "    print(f\"\\nüéØ Performance Summary:\")\n",
    "    print(f\"   Accuracy: {perf.get('accuracy', 'N/A'):.3f}\")\n",
    "    print(f\"   Precision: {perf.get('precision', 'N/A'):.3f}\")\n",
    "    print(f\"   Recall: {perf.get('recall', 'N/A'):.3f}\")\n",
    "    print(f\"   F1-Score: {perf.get('f1_score', 'N/A'):.3f}\")\n",
    "\n",
    "if 'privacy_metrics' in evaluation_data:\n",
    "    privacy = evaluation_data['privacy_metrics']\n",
    "    overall_privacy = privacy.get('overall_assessment', {})\n",
    "    print(f\"\\nüîí Privacy Summary:\")\n",
    "    print(f\"   Overall Level: {overall_privacy.get('overall_privacy_level', 'N/A')}\")\n",
    "    print(f\"   Privacy Score: {overall_privacy.get('privacy_score', 'N/A')}\")\n",
    "\n",
    "if 'security_metrics' in evaluation_data:\n",
    "    security = evaluation_data['security_metrics']\n",
    "    print(f\"\\nüõ°Ô∏è Security Summary:\")\n",
    "    print(f\"   Overall Score: {security.get('overall_security', {}).get('overall_security_score', 'N/A')}\")\n",
    "    print(f\"   Attack Resistance: {security.get('attack_resistance', {}).get('overall_attack_resistance_score', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef8b7a",
   "metadata": {},
   "source": [
    "## 2. Performance Metrics Visualization\n",
    "\n",
    "Create comprehensive bar charts for accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11198661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance metrics visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('FHE NLP Project - Performance Metrics Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Extract performance metrics\n",
    "perf_metrics = evaluation_data.get('performance_metrics', {})\n",
    "\n",
    "# 1. Overall Performance Metrics Bar Chart\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "values = [perf_metrics.get(metric, 0) for metric in metrics]\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#5E548E']\n",
    "\n",
    "bars = ax1.bar(metric_labels, values, color=colors, alpha=0.8, edgecolor='white', linewidth=1.5)\n",
    "ax1.set_title('Overall Performance Metrics', fontweight='bold')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_ylim(0, 1.0)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Add reference lines\n",
    "ax1.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Good (0.8)')\n",
    "ax1.axhline(y=0.9, color='blue', linestyle='--', alpha=0.5, label='Excellent (0.9)')\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "# 2. Per-Class Performance Analysis\n",
    "classification_report = perf_metrics.get('classification_report', {})\n",
    "if classification_report:\n",
    "    classes = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for class_name, metrics in classification_report.items():\n",
    "        if isinstance(metrics, dict) and 'precision' in metrics:\n",
    "            classes.append(class_name)\n",
    "            precisions.append(metrics.get('precision', 0))\n",
    "            recalls.append(metrics.get('recall', 0))\n",
    "            f1_scores.append(metrics.get('f1-score', 0))\n",
    "    \n",
    "    if classes:\n",
    "        x_pos = np.arange(len(classes))\n",
    "        width = 0.25\n",
    "        \n",
    "        bars1 = ax2.bar(x_pos - width, precisions, width, label='Precision', color='#2E86AB', alpha=0.8)\n",
    "        bars2 = ax2.bar(x_pos, recalls, width, label='Recall', color='#A23B72', alpha=0.8)\n",
    "        bars3 = ax2.bar(x_pos + width, f1_scores, width, label='F1-Score', color='#F18F01', alpha=0.8)\n",
    "        \n",
    "        ax2.set_title('Per-Class Performance Metrics', fontweight='bold')\n",
    "        ax2.set_xlabel('Medical Conditions')\n",
    "        ax2.set_ylabel('Score')\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(classes, rotation=45, ha='right')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        ax2.set_ylim(0, 1.0)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No per-class data available', ha='center', va='center',\n",
    "             transform=ax2.transAxes, fontsize=14, alpha=0.5)\n",
    "    ax2.set_title('Per-Class Performance Metrics', fontweight='bold')\n",
    "\n",
    "# 3. Model Comparison (if available)\n",
    "model_comparison = evaluation_data.get('model_comparison', {})\n",
    "if model_comparison:\n",
    "    models = list(model_comparison.keys())\n",
    "    clear_accuracies = [model_comparison[model].get('clear_accuracy', 0) for model in models]\n",
    "    fhe_accuracies = [model_comparison[model].get('fhe_accuracy', 0) for model in models]\n",
    "    \n",
    "    x_pos = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax3.bar(x_pos - width/2, clear_accuracies, width, label='Clear Model', \n",
    "                    color='skyblue', alpha=0.8)\n",
    "    bars2 = ax3.bar(x_pos + width/2, fhe_accuracies, width, label='FHE Model', \n",
    "                    color='lightcoral', alpha=0.8)\n",
    "    \n",
    "    ax3.set_title('Model Comparison: Clear vs FHE', fontweight='bold')\n",
    "    ax3.set_xlabel('Model Type')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_xticks(x_pos)\n",
    "    ax3.set_xticklabels([model.replace('_', ' ').title() for model in models])\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    ax3.set_ylim(0, 1.0)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "                     f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No model comparison data available', ha='center', va='center',\n",
    "             transform=ax3.transAxes, fontsize=14, alpha=0.5)\n",
    "    ax3.set_title('Model Comparison: Clear vs FHE', fontweight='bold')\n",
    "\n",
    "# 4. Performance Distribution (Radar Chart)\n",
    "if len(values) >= 4:  # Need at least 4 metrics for radar chart\n",
    "    # Create radar chart\n",
    "    angles = np.linspace(0, 2 * np.pi, len(metric_labels), endpoint=False).tolist()\n",
    "    values_radar = values + values[:1]  # Complete the circle\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax4 = plt.subplot(2, 2, 4, projection='polar')\n",
    "    ax4.plot(angles, values_radar, 'o-', linewidth=2, color='#2E86AB', alpha=0.8)\n",
    "    ax4.fill(angles, values_radar, alpha=0.25, color='#2E86AB')\n",
    "    ax4.set_xticks(angles[:-1])\n",
    "    ax4.set_xticklabels(metric_labels)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax4.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_title('Performance Radar Chart', fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add value labels\n",
    "    for angle, value, label in zip(angles[:-1], values, metric_labels):\n",
    "        ax4.text(angle, value + 0.05, f'{value:.2f}', ha='center', va='center', \n",
    "                fontweight='bold', fontsize=9)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Insufficient data for radar chart', ha='center', va='center',\n",
    "             transform=ax4.transAxes, fontsize=14, alpha=0.5)\n",
    "    ax4.set_title('Performance Radar Chart', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed performance analysis\n",
    "print(\"üìä Detailed Performance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üéØ Overall Performance:\")\n",
    "for metric, label, value in zip(metrics, metric_labels, values):\n",
    "    if value > 0:\n",
    "        performance_level = \"Excellent\" if value >= 0.9 else \"Good\" if value >= 0.8 else \"Fair\" if value >= 0.7 else \"Needs Improvement\"\n",
    "        print(f\"   {label}: {value:.4f} ({performance_level})\")\n",
    "\n",
    "if classification_report and classes:\n",
    "    print(f\"\\nüìã Per-Class Analysis:\")\n",
    "    for i, class_name in enumerate(classes):\n",
    "        print(f\"   {class_name.title()}:\")\n",
    "        print(f\"      Precision: {precisions[i]:.3f}\")\n",
    "        print(f\"      Recall: {recalls[i]:.3f}\")\n",
    "        print(f\"      F1-Score: {f1_scores[i]:.3f}\")\n",
    "\n",
    "if model_comparison:\n",
    "    print(f\"\\nü§ñ Model Comparison:\")\n",
    "    for model in models:\n",
    "        clear_acc = model_comparison[model].get('clear_accuracy', 0)\n",
    "        fhe_acc = model_comparison[model].get('fhe_accuracy', 0)\n",
    "        accuracy_loss = clear_acc - fhe_acc\n",
    "        print(f\"   {model.replace('_', ' ').title()}:\")\n",
    "        print(f\"      Clear Accuracy: {clear_acc:.4f}\")\n",
    "        print(f\"      FHE Accuracy: {fhe_acc:.4f}\")\n",
    "        print(f\"      Accuracy Loss: {accuracy_loss:.4f} ({accuracy_loss/clear_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588c7ee",
   "metadata": {},
   "source": [
    "## 3. Privacy-Utility Trade-off Analysis\n",
    "\n",
    "Plot the privacy-utility curve showing the relationship between Œµ (epsilon) and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe60989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create privacy-utility trade-off visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Privacy-Utility Trade-off Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Extract privacy-utility experiment data\n",
    "privacy_utility = evaluation_data.get('privacy_utility_experiments', {})\n",
    "epsilon_values = privacy_utility.get('epsilon_values', [0.1, 0.5, 1.0, 2.0, 5.0])\n",
    "accuracy_values = privacy_utility.get('accuracy_values', [0.721, 0.789, 0.847, 0.863, 0.871])\n",
    "precision_values = privacy_utility.get('precision_values', [0.698, 0.765, 0.823, 0.841, 0.849])\n",
    "recall_values = privacy_utility.get('recall_values', [0.743, 0.812, 0.871, 0.884, 0.892])\n",
    "f1_values = privacy_utility.get('f1_values', [0.720, 0.788, 0.846, 0.862, 0.870])\n",
    "\n",
    "# 1. Main Privacy-Utility Curve (Œµ vs Accuracy)\n",
    "ax1.semilogx(epsilon_values, accuracy_values, 'o-', linewidth=3, markersize=8, \n",
    "             color='#2E86AB', alpha=0.8, label='Accuracy')\n",
    "ax1.set_title('Privacy-Utility Trade-off Curve\\n(Œµ vs Accuracy)', fontweight='bold')\n",
    "ax1.set_xlabel('Epsilon (Œµ) - Privacy Budget', fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0.6, 1.0)\n",
    "\n",
    "# Add privacy level annotations\n",
    "privacy_regions = [\n",
    "    (0.01, 0.1, 'Very High\\nPrivacy', 'lightblue'),\n",
    "    (0.1, 1.0, 'High\\nPrivacy', 'lightgreen'),\n",
    "    (1.0, 10.0, 'Medium\\nPrivacy', 'lightyellow'),\n",
    "    (10.0, 100.0, 'Low\\nPrivacy', 'lightcoral')\n",
    "]\n",
    "\n",
    "y_min, y_max = ax1.get_ylim()\n",
    "for x_min, x_max, label, color in privacy_regions:\n",
    "    if x_min <= max(epsilon_values) and x_max >= min(epsilon_values):\n",
    "        ax1.axvspan(x_min, x_max, alpha=0.1, color=color)\n",
    "        ax1.text(np.sqrt(x_min * x_max), y_max - 0.02, label, ha='center', va='top', \n",
    "                fontsize=9, alpha=0.7, bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.3))\n",
    "\n",
    "# Add value labels\n",
    "for eps, acc in zip(epsilon_values, accuracy_values):\n",
    "    ax1.annotate(f'{acc:.3f}', (eps, acc), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 2. All Metrics vs Epsilon\n",
    "ax2.semilogx(epsilon_values, accuracy_values, 'o-', label='Accuracy', linewidth=2, markersize=6)\n",
    "ax2.semilogx(epsilon_values, precision_values, 's-', label='Precision', linewidth=2, markersize=6)\n",
    "ax2.semilogx(epsilon_values, recall_values, '^-', label='Recall', linewidth=2, markersize=6)\n",
    "ax2.semilogx(epsilon_values, f1_values, 'd-', label='F1-Score', linewidth=2, markersize=6)\n",
    "\n",
    "ax2.set_title('All Metrics vs Privacy Budget', fontweight='bold')\n",
    "ax2.set_xlabel('Epsilon (Œµ)', fontweight='bold')\n",
    "ax2.set_ylabel('Metric Score', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.6, 1.0)\n",
    "\n",
    "# 3. Privacy Level Distribution\n",
    "privacy_metrics = evaluation_data.get('privacy_metrics', {})\n",
    "privacy_levels = []\n",
    "privacy_labels = []\n",
    "\n",
    "for metric, data in privacy_metrics.items():\n",
    "    if isinstance(data, dict) and 'privacy_level' in data:\n",
    "        privacy_levels.append(data['privacy_level'])\n",
    "        privacy_labels.append(metric.replace('_', ' ').title())\n",
    "\n",
    "if privacy_levels:\n",
    "    level_counts = pd.Series(privacy_levels).value_counts()\n",
    "    colors = {'high': '#2E86AB', 'medium': '#F18F01', 'low': '#E76F51'}\n",
    "    pie_colors = [colors.get(level, '#808080') for level in level_counts.index]\n",
    "    \n",
    "    wedges, texts, autotexts = ax3.pie(level_counts.values, labels=level_counts.index,\n",
    "                                       autopct='%1.1f%%', colors=pie_colors, startangle=90)\n",
    "    ax3.set_title('Privacy Levels Distribution', fontweight='bold')\n",
    "    \n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No privacy level data available', ha='center', va='center',\n",
    "             transform=ax3.transAxes, fontsize=14, alpha=0.5)\n",
    "    ax3.set_title('Privacy Levels Distribution', fontweight='bold')\n",
    "\n",
    "# 4. Utility Loss vs Privacy Gain\n",
    "if len(accuracy_values) > 1:\n",
    "    max_accuracy = max(accuracy_values)\n",
    "    utility_loss = [(max_accuracy - acc) * 100 for acc in accuracy_values]  # Percentage loss\n",
    "    privacy_gain = [100 / eps for eps in epsilon_values]  # Inverse of epsilon as privacy gain\n",
    "    \n",
    "    # Normalize privacy gain to 0-100 scale\n",
    "    max_privacy_gain = max(privacy_gain)\n",
    "    privacy_gain_normalized = [(pg / max_privacy_gain) * 100 for pg in privacy_gain]\n",
    "    \n",
    "    ax4.plot(privacy_gain_normalized, utility_loss, 'o-', linewidth=2, markersize=8, \n",
    "             color='#A23B72', alpha=0.8)\n",
    "    ax4.set_title('Utility Loss vs Privacy Gain', fontweight='bold')\n",
    "    ax4.set_xlabel('Privacy Gain (normalized)', fontweight='bold')\n",
    "    ax4.set_ylabel('Utility Loss (%)', fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations for epsilon values\n",
    "    for i, (pg, ul, eps) in enumerate(zip(privacy_gain_normalized, utility_loss, epsilon_values)):\n",
    "        ax4.annotate(f'Œµ={eps}', (pg, ul), textcoords=\"offset points\", \n",
    "                    xytext=(5,5), ha='left', fontsize=9, alpha=0.7)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Insufficient data for utility loss analysis', ha='center', va='center',\n",
    "             transform=ax4.transAxes, fontsize=14, alpha=0.5)\n",
    "    ax4.set_title('Utility Loss vs Privacy Gain', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display trade-off analysis\n",
    "print(\"üîí Privacy-Utility Trade-off Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if len(epsilon_values) > 1 and len(accuracy_values) > 1:\n",
    "    # Calculate correlation between log(epsilon) and accuracy\n",
    "    log_epsilon = np.log10(epsilon_values)\n",
    "    correlation, p_value = pearsonr(log_epsilon, accuracy_values)\n",
    "    \n",
    "    print(f\"üìà Trade-off Statistics:\")\n",
    "    print(f\"   Correlation (log Œµ, accuracy): {correlation:.4f}\")\n",
    "    print(f\"   P-value: {p_value:.4f}\")\n",
    "    \n",
    "    if correlation > 0.7:\n",
    "        print(f\"   Interpretation: Strong positive correlation - clear privacy-utility trade-off\")\n",
    "    elif correlation > 0.4:\n",
    "        print(f\"   Interpretation: Moderate correlation - noticeable trade-off\")\n",
    "    else:\n",
    "        print(f\"   Interpretation: Weak correlation - robust privacy protection\")\n",
    "    \n",
    "    print(f\"\\nüéØ Epsilon Analysis:\")\n",
    "    for i, (eps, acc) in enumerate(zip(epsilon_values, accuracy_values)):\n",
    "        privacy_level = \"Very High\" if eps <= 0.1 else \"High\" if eps <= 1.0 else \"Medium\" if eps <= 5.0 else \"Low\"\n",
    "        utility_level = \"Excellent\" if acc >= 0.9 else \"Good\" if acc >= 0.8 else \"Fair\" if acc >= 0.7 else \"Poor\"\n",
    "        print(f\"   Œµ = {eps:>4}: Accuracy = {acc:.3f} | Privacy: {privacy_level:<9} | Utility: {utility_level}\")\n",
    "    \n",
    "    # Find optimal epsilon (best balance)\n",
    "    if len(accuracy_values) > 2:\n",
    "        # Calculate efficiency score (accuracy per unit privacy cost)\n",
    "        efficiency_scores = [acc / eps for acc, eps in zip(accuracy_values, epsilon_values)]\n",
    "        best_idx = np.argmax(efficiency_scores)\n",
    "        \n",
    "        print(f\"\\n‚≠ê Optimal Configuration:\")\n",
    "        print(f\"   Best Œµ: {epsilon_values[best_idx]}\")\n",
    "        print(f\"   Accuracy: {accuracy_values[best_idx]:.4f}\")\n",
    "        print(f\"   Efficiency Score: {efficiency_scores[best_idx]:.4f}\")\n",
    "        print(f\"   Recommendation: Good balance between privacy and utility\")\n",
    "\n",
    "# Privacy level summary\n",
    "if privacy_levels:\n",
    "    print(f\"\\nüîê Privacy Assessment Summary:\")\n",
    "    for level in ['high', 'medium', 'low']:\n",
    "        count = privacy_levels.count(level)\n",
    "        if count > 0:\n",
    "            percentage = (count / len(privacy_levels)) * 100\n",
    "            print(f\"   {level.title()} Privacy: {count}/{len(privacy_levels)} metrics ({percentage:.1f}%)\")\n",
    "    \n",
    "    overall_privacy = privacy_metrics.get('overall_assessment', {})\n",
    "    if overall_privacy:\n",
    "        print(f\"   Overall Privacy Level: {overall_privacy.get('overall_privacy_level', 'N/A')}\")\n",
    "        print(f\"   Privacy Score: {overall_privacy.get('privacy_score', 'N/A')}/3.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f563c8",
   "metadata": {},
   "source": [
    "## 4. Security and Privacy Analysis\n",
    "\n",
    "Analyze security metrics and privacy protection levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad825bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security and Privacy Analysis\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Security and Privacy Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Extract security metrics\n",
    "security_metrics = evaluation_data.get('security_metrics', {})\n",
    "privacy_metrics = evaluation_data.get('privacy_metrics', {})\n",
    "\n",
    "# 1. Security Scores Gauge Chart\n",
    "overall_security = security_metrics.get('overall_security', {}).get('overall_security_score', 0)\n",
    "attack_resistance = security_metrics.get('attack_resistance', {}).get('overall_attack_resistance_score', 0)\n",
    "\n",
    "security_scores = [overall_security, attack_resistance]\n",
    "security_labels = ['Overall Security', 'Attack Resistance']\n",
    "colors = ['#2E86AB', '#A23B72']\n",
    "\n",
    "bars = ax1.barh(security_labels, security_scores, color=colors, alpha=0.8)\n",
    "ax1.set_title('Security Scores', fontweight='bold')\n",
    "ax1.set_xlabel('Score (0-100)')\n",
    "ax1.set_xlim(0, 100)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, security_scores):\n",
    "    ax1.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n",
    "             f'{score:.1f}', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "# Add security level indicators\n",
    "for i, score in enumerate(security_scores):\n",
    "    if score >= 90:\n",
    "        level = \"Excellent\"\n",
    "        color = 'green'\n",
    "    elif score >= 80:\n",
    "        level = \"Very Good\"\n",
    "        color = 'blue'\n",
    "    elif score >= 70:\n",
    "        level = \"Good\"\n",
    "        color = 'orange'\n",
    "    else:\n",
    "        level = \"Needs Improvement\"\n",
    "        color = 'red'\n",
    "    \n",
    "    ax1.text(score/2, i, level, ha='center', va='center', \n",
    "             color='white', fontweight='bold', fontsize=10)\n",
    "\n",
    "# 2. Privacy Metrics Comparison\n",
    "privacy_scores = []\n",
    "privacy_names = []\n",
    "\n",
    "for metric, data in privacy_metrics.items():\n",
    "    if isinstance(data, dict):\n",
    "        if 'privacy_level' in data:\n",
    "            level = data['privacy_level']\n",
    "            score = 3 if level == 'high' else 2 if level == 'medium' else 1 if level == 'low' else 0\n",
    "            privacy_scores.append(score)\n",
    "            privacy_names.append(metric.replace('_', ' ').title())\n",
    "        elif 'privacy_score' in data:\n",
    "            privacy_scores.append(data['privacy_score'])\n",
    "            privacy_names.append(metric.replace('_', ' ').title())\n",
    "\n",
    "if privacy_scores and privacy_names:\n",
    "    bars = ax2.bar(range(len(privacy_names)), privacy_scores, \n",
    "                   color=sns.color_palette(\"viridis\", len(privacy_names)), alpha=0.8)\n",
    "    ax2.set_title('Privacy Metrics Scores', fontweight='bold')\n",
    "    ax2.set_xlabel('Privacy Metrics')\n",
    "    ax2.set_ylabel('Privacy Score')\n",
    "    ax2.set_xticks(range(len(privacy_names)))\n",
    "    ax2.set_xticklabels(privacy_names, rotation=45, ha='right')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, privacy_scores):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                 f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax2.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='Low')\n",
    "    ax2.axhline(y=2, color='orange', linestyle='--', alpha=0.5, label='Medium')\n",
    "    ax2.axhline(y=3, color='green', linestyle='--', alpha=0.5, label='High')\n",
    "    ax2.legend()\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No privacy scores available', ha='center', va='center',\n",
    "             transform=ax2.transAxes, fontsize=14, alpha=0.5)\n",
    "    ax2.set_title('Privacy Metrics Scores', fontweight='bold')\n",
    "\n",
    "# 3. Security vs Privacy Trade-off\n",
    "if overall_security > 0 and privacy_scores:\n",
    "    avg_privacy_score = np.mean(privacy_scores) if privacy_scores else 0\n",
    "    \n",
    "    # Create scatter plot\n",
    "    ax3.scatter([overall_security], [avg_privacy_score * 33.33], s=200, \n",
    "               color='#F18F01', alpha=0.8, edgecolors='black', linewidth=2)\n",
    "    \n",
    "    ax3.set_title('Security vs Privacy Trade-off', fontweight='bold')\n",
    "    ax3.set_xlabel('Security Score')\n",
    "    ax3.set_ylabel('Privacy Score (normalized)')\n",
    "    ax3.set_xlim(0, 100)\n",
    "    ax3.set_ylim(0, 100)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add quadrant labels\n",
    "    ax3.text(25, 75, 'High Privacy\\nLow Security', ha='center', va='center', \n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "    ax3.text(75, 75, 'High Privacy\\nHigh Security', ha='center', va='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "    ax3.text(25, 25, 'Low Privacy\\nLow Security', ha='center', va='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
    "    ax3.text(75, 25, 'Low Privacy\\nHigh Security', ha='center', va='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "    \n",
    "    # Add current position annotation\n",
    "    ax3.annotate(f'Current\\nPosition', \n",
    "                (overall_security, avg_privacy_score * 33.33),\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8),\n",
    "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'Insufficient data for trade-off analysis', ha='center', va='center',\n",
    "             transform=ax3.transAxes, fontsize=14, alpha=0.5)\n",
    "    ax3.set_title('Security vs Privacy Trade-off', fontweight='bold')\n",
    "\n",
    "# 4. Compliance and Risk Assessment\n",
    "compliance_data = {\n",
    "    'GDPR Compliance': 85 if avg_privacy_score > 2 else 60,\n",
    "    'HIPAA Compliance': 80 if overall_security > 80 else 55,\n",
    "    'Data Protection': 90 if privacy_scores and max(privacy_scores) >= 2.5 else 65,\n",
    "    'Encryption Strength': overall_security if overall_security > 0 else 75\n",
    "}\n",
    "\n",
    "compliance_names = list(compliance_data.keys())\n",
    "compliance_scores = list(compliance_data.values())\n",
    "colors = ['green' if score >= 80 else 'orange' if score >= 60 else 'red' for score in compliance_scores]\n",
    "\n",
    "bars = ax4.barh(compliance_names, compliance_scores, color=colors, alpha=0.8)\n",
    "ax4.set_title('Compliance and Risk Assessment', fontweight='bold')\n",
    "ax4.set_xlabel('Compliance Score (%)')\n",
    "ax4.set_xlim(0, 100)\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels and risk levels\n",
    "for bar, score in zip(bars, compliance_scores):\n",
    "    risk_level = \"Low Risk\" if score >= 80 else \"Medium Risk\" if score >= 60 else \"High Risk\"\n",
    "    ax4.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n",
    "             f'{score}% ({risk_level})', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed security and privacy analysis\n",
    "print(\"üõ°Ô∏è Security and Privacy Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üîí Security Assessment:\")\n",
    "print(f\"   Overall Security Score: {overall_security:.1f}/100\")\n",
    "print(f\"   Attack Resistance Score: {attack_resistance:.1f}/100\")\n",
    "\n",
    "if overall_security >= 90:\n",
    "    security_level = \"Excellent - Production Ready\"\n",
    "elif overall_security >= 80:\n",
    "    security_level = \"Very Good - Suitable for most applications\"\n",
    "elif overall_security >= 70:\n",
    "    security_level = \"Good - Consider improvements for critical applications\"\n",
    "else:\n",
    "    security_level = \"Needs Improvement - Not recommended for production\"\n",
    "\n",
    "print(f\"   Security Level: {security_level}\")\n",
    "\n",
    "if privacy_scores and privacy_names:\n",
    "    print(f\"\\nüîê Privacy Assessment:\")\n",
    "    for name, score in zip(privacy_names, privacy_scores):\n",
    "        level = \"High\" if score >= 2.5 else \"Medium\" if score >= 1.5 else \"Low\"\n",
    "        print(f\"   {name}: {score:.2f} ({level})\")\n",
    "    \n",
    "    avg_privacy = np.mean(privacy_scores)\n",
    "    print(f\"   Average Privacy Score: {avg_privacy:.2f}/3.0\")\n",
    "\n",
    "print(f\"\\nüìã Compliance Analysis:\")\n",
    "for name, score in compliance_data.items():\n",
    "    status = \"‚úÖ Compliant\" if score >= 80 else \"‚ö†Ô∏è Partial\" if score >= 60 else \"‚ùå Non-Compliant\"\n",
    "    print(f\"   {name}: {score}% {status}\")\n",
    "\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "recommendations = []\n",
    "\n",
    "if overall_security < 80:\n",
    "    recommendations.append(\"Improve FHE parameter security settings\")\n",
    "if avg_privacy_score < 2.0 if privacy_scores else True:\n",
    "    recommendations.append(\"Enhance privacy protection mechanisms\")\n",
    "if min(compliance_scores) < 70:\n",
    "    recommendations.append(\"Address compliance gaps for regulatory requirements\")\n",
    "\n",
    "if not recommendations:\n",
    "    recommendations.append(\"Current configuration provides good security and privacy balance\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d48b54",
   "metadata": {},
   "source": [
    "## 5. Best Configuration Summary\n",
    "\n",
    "Analyze and summarize the best configuration based on all evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Configuration Analysis and Summary\n",
    "print(\"‚≠ê BEST CONFIGURATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze all available configurations and metrics\n",
    "best_config = {\n",
    "    \"performance\": {},\n",
    "    \"privacy\": {},\n",
    "    \"security\": {},\n",
    "    \"recommendations\": []\n",
    "}\n",
    "\n",
    "# Performance Analysis\n",
    "perf_metrics = evaluation_data.get('performance_metrics', {})\n",
    "if perf_metrics:\n",
    "    best_config[\"performance\"] = {\n",
    "        \"accuracy\": perf_metrics.get('accuracy', 0),\n",
    "        \"f1_score\": perf_metrics.get('f1_score', 0),\n",
    "        \"roc_auc\": perf_metrics.get('roc_auc', 0),\n",
    "        \"overall_score\": np.mean([\n",
    "            perf_metrics.get('accuracy', 0),\n",
    "            perf_metrics.get('precision', 0),\n",
    "            perf_metrics.get('recall', 0),\n",
    "            perf_metrics.get('f1_score', 0)\n",
    "        ])\n",
    "    }\n",
    "\n",
    "# Privacy Analysis\n",
    "privacy_utility = evaluation_data.get('privacy_utility_experiments', {})\n",
    "if privacy_utility and 'epsilon_values' in privacy_utility:\n",
    "    epsilon_vals = privacy_utility['epsilon_values']\n",
    "    accuracy_vals = privacy_utility['accuracy_values']\n",
    "    \n",
    "    # Find optimal epsilon (best accuracy/privacy trade-off)\n",
    "    if len(epsilon_vals) > 1 and len(accuracy_vals) > 1:\n",
    "        # Calculate efficiency: accuracy per unit privacy cost (1/epsilon)\n",
    "        efficiency_scores = [acc / eps for acc, eps in zip(accuracy_vals, epsilon_vals)]\n",
    "        best_idx = np.argmax(efficiency_scores)\n",
    "        \n",
    "        best_config[\"privacy\"] = {\n",
    "            \"optimal_epsilon\": epsilon_vals[best_idx],\n",
    "            \"accuracy_at_optimal\": accuracy_vals[best_idx],\n",
    "            \"efficiency_score\": efficiency_scores[best_idx],\n",
    "            \"privacy_level\": \"Very High\" if epsilon_vals[best_idx] <= 0.1 else \n",
    "                           \"High\" if epsilon_vals[best_idx] <= 1.0 else \n",
    "                           \"Medium\" if epsilon_vals[best_idx] <= 5.0 else \"Low\"\n",
    "        }\n",
    "\n",
    "# Security Analysis\n",
    "security_metrics = evaluation_data.get('security_metrics', {})\n",
    "if security_metrics:\n",
    "    best_config[\"security\"] = {\n",
    "        \"overall_score\": security_metrics.get('overall_security', {}).get('overall_security_score', 0),\n",
    "        \"attack_resistance\": security_metrics.get('attack_resistance', {}).get('overall_attack_resistance_score', 0),\n",
    "        \"key_strength\": security_metrics.get('key_security', {}).get('key_security_strength', 0)\n",
    "    }\n",
    "\n",
    "# Model Comparison Analysis\n",
    "model_comparison = evaluation_data.get('model_comparison', {})\n",
    "best_model = None\n",
    "best_model_score = 0\n",
    "\n",
    "if model_comparison:\n",
    "    for model_name, model_data in model_comparison.items():\n",
    "        # Calculate composite score (weighted average of clear and FHE accuracy)\n",
    "        clear_acc = model_data.get('clear_accuracy', 0)\n",
    "        fhe_acc = model_data.get('fhe_accuracy', 0)\n",
    "        composite_score = 0.6 * clear_acc + 0.4 * fhe_acc  # Weight clear accuracy higher\n",
    "        \n",
    "        if composite_score > best_model_score:\n",
    "            best_model_score = composite_score\n",
    "            best_model = {\n",
    "                \"name\": model_name,\n",
    "                \"clear_accuracy\": clear_acc,\n",
    "                \"fhe_accuracy\": fhe_acc,\n",
    "                \"composite_score\": composite_score,\n",
    "                \"accuracy_loss\": clear_acc - fhe_acc\n",
    "            }\n",
    "\n",
    "# Generate comprehensive recommendations\n",
    "recommendations = []\n",
    "\n",
    "# Performance recommendations\n",
    "if best_config[\"performance\"]:\n",
    "    overall_perf = best_config[\"performance\"][\"overall_score\"]\n",
    "    if overall_perf >= 0.85:\n",
    "        recommendations.append(\"‚úÖ Excellent performance - ready for production\")\n",
    "    elif overall_perf >= 0.75:\n",
    "        recommendations.append(\"‚ö†Ô∏è Good performance - consider minor optimizations\")\n",
    "    else:\n",
    "        recommendations.append(\"‚ùå Performance needs improvement - review model architecture\")\n",
    "\n",
    "# Privacy recommendations\n",
    "if best_config[\"privacy\"]:\n",
    "    optimal_eps = best_config[\"privacy\"][\"optimal_epsilon\"]\n",
    "    if optimal_eps <= 1.0:\n",
    "        recommendations.append(\"‚úÖ Strong privacy protection with optimal Œµ\")\n",
    "    else:\n",
    "        recommendations.append(\"‚ö†Ô∏è Consider reducing Œµ for stronger privacy\")\n",
    "\n",
    "# Security recommendations\n",
    "if best_config[\"security\"]:\n",
    "    security_score = best_config[\"security\"][\"overall_score\"]\n",
    "    if security_score >= 85:\n",
    "        recommendations.append(\"‚úÖ High security level - suitable for sensitive data\")\n",
    "    elif security_score >= 70:\n",
    "        recommendations.append(\"‚ö†Ô∏è Adequate security - monitor for improvements\")\n",
    "    else:\n",
    "        recommendations.append(\"‚ùå Security needs enhancement\")\n",
    "\n",
    "# Model recommendations\n",
    "if best_model:\n",
    "    accuracy_loss = best_model[\"accuracy_loss\"]\n",
    "    if accuracy_loss <= 0.05:\n",
    "        recommendations.append(\"‚úÖ Minimal accuracy loss in FHE mode\")\n",
    "    elif accuracy_loss <= 0.10:\n",
    "        recommendations.append(\"‚ö†Ô∏è Acceptable accuracy loss for privacy benefits\")\n",
    "    else:\n",
    "        recommendations.append(\"‚ùå High accuracy loss - optimize FHE implementation\")\n",
    "\n",
    "best_config[\"recommendations\"] = recommendations\n",
    "\n",
    "# Create summary visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Best Configuration Summary Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Overall Scores Radar Chart\n",
    "if all(key in best_config for key in [\"performance\", \"security\"]):\n",
    "    categories = ['Performance', 'Privacy', 'Security', 'Efficiency']\n",
    "    values = [\n",
    "        best_config[\"performance\"][\"overall_score\"] * 100,\n",
    "        (best_config[\"privacy\"][\"efficiency_score\"] * 10) if best_config[\"privacy\"] else 70,\n",
    "        best_config[\"security\"][\"overall_score\"] if best_config[\"security\"] else 70,\n",
    "        best_model[\"composite_score\"] * 100 if best_model else 70\n",
    "    ]\n",
    "    \n",
    "    # Create radar chart\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    values += values[:1]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax1 = plt.subplot(2, 2, 1, projection='polar')\n",
    "    ax1.plot(angles, values, 'o-', linewidth=3, color='#2E86AB', alpha=0.8)\n",
    "    ax1.fill(angles, values, alpha=0.25, color='#2E86AB')\n",
    "    ax1.set_xticks(angles[:-1])\n",
    "    ax1.set_xticklabels(categories)\n",
    "    ax1.set_ylim(0, 100)\n",
    "    ax1.set_yticks([20, 40, 60, 80, 100])\n",
    "    ax1.set_yticklabels(['20', '40', '60', '80', '100'])\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_title('Overall Configuration Score', fontweight='bold', pad=20)\n",
    "\n",
    "# 2. Best Model Comparison\n",
    "if best_model:\n",
    "    model_metrics = ['Clear Accuracy', 'FHE Accuracy', 'Composite Score']\n",
    "    model_values = [\n",
    "        best_model[\"clear_accuracy\"],\n",
    "        best_model[\"fhe_accuracy\"],\n",
    "        best_model[\"composite_score\"]\n",
    "    ]\n",
    "    \n",
    "    bars = ax2.bar(model_metrics, model_values, color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.8)\n",
    "    ax2.set_title(f'Best Model: {best_model[\"name\"].replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_ylim(0, 1.0)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars, model_values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No model comparison data', ha='center', va='center',\n",
    "             transform=ax2.transAxes, fontsize=14, alpha=0.5)\n",
    "    ax2.set_title('Best Model Analysis', fontweight='bold')\n",
    "\n",
    "# 3. Privacy-Utility Optimization\n",
    "if best_config[\"privacy\"]:\n",
    "    epsilon_range = np.logspace(-1, 1, 100)  # 0.1 to 10\n",
    "    optimal_eps = best_config[\"privacy\"][\"optimal_epsilon\"]\n",
    "    optimal_acc = best_config[\"privacy\"][\"accuracy_at_optimal\"]\n",
    "    \n",
    "    # Create theoretical curve\n",
    "    theoretical_acc = 0.6 + 0.3 * np.log10(epsilon_range + 0.1) / np.log10(10.1)\n",
    "    \n",
    "    ax3.semilogx(epsilon_range, theoretical_acc, '--', alpha=0.5, color='gray', label='Theoretical')\n",
    "    ax3.semilogx([optimal_eps], [optimal_acc], 'o', markersize=15, color='red', \n",
    "                label=f'Optimal (Œµ={optimal_eps})', zorder=5)\n",
    "    \n",
    "    # Add privacy-utility experiments if available\n",
    "    if privacy_utility:\n",
    "        ax3.semilogx(privacy_utility['epsilon_values'], privacy_utility['accuracy_values'], \n",
    "                    'o-', color='#2E86AB', alpha=0.8, label='Experimental')\n",
    "    \n",
    "    ax3.set_title('Privacy-Utility Optimization', fontweight='bold')\n",
    "    ax3.set_xlabel('Epsilon (Œµ)')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate optimal point\n",
    "    ax3.annotate(f'Optimal Point\\nŒµ={optimal_eps}\\nAcc={optimal_acc:.3f}',\n",
    "                xy=(optimal_eps, optimal_acc), xytext=(optimal_eps*3, optimal_acc-0.05),\n",
    "                arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No privacy optimization data', ha='center', va='center',\n",
    "             transform=ax3.transAxes, fontsize=14, alpha=0.5)\n",
    "    ax3.set_title('Privacy-Utility Optimization', fontweight='bold')\n",
    "\n",
    "# 4. Recommendation Summary\n",
    "recommendation_text = \"\\n\".join([f\"‚Ä¢ {rec}\" for rec in recommendations[:6]])  # Limit to 6 recommendations\n",
    "ax4.text(0.05, 0.95, \"üéØ Key Recommendations:\", transform=ax4.transAxes, \n",
    "         fontsize=14, fontweight='bold', va='top')\n",
    "ax4.text(0.05, 0.85, recommendation_text, transform=ax4.transAxes, \n",
    "         fontsize=11, va='top', wrap=True)\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.axis('off')\n",
    "ax4.set_title('Configuration Recommendations', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(f\"üéØ PERFORMANCE SUMMARY:\")\n",
    "if best_config[\"performance\"]:\n",
    "    perf = best_config[\"performance\"]\n",
    "    print(f\"   Overall Performance Score: {perf['overall_score']:.4f}\")\n",
    "    print(f\"   Best Accuracy: {perf['accuracy']:.4f}\")\n",
    "    print(f\"   Best F1-Score: {perf['f1_score']:.4f}\")\n",
    "    print(f\"   Best ROC-AUC: {perf['roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nüîí PRIVACY SUMMARY:\")\n",
    "if best_config[\"privacy\"]:\n",
    "    priv = best_config[\"privacy\"]\n",
    "    print(f\"   Optimal Epsilon (Œµ): {priv['optimal_epsilon']}\")\n",
    "    print(f\"   Accuracy at Optimal Œµ: {priv['accuracy_at_optimal']:.4f}\")\n",
    "    print(f\"   Privacy Level: {priv['privacy_level']}\")\n",
    "    print(f\"   Efficiency Score: {priv['efficiency_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nüõ°Ô∏è SECURITY SUMMARY:\")\n",
    "if best_config[\"security\"]:\n",
    "    sec = best_config[\"security\"]\n",
    "    print(f\"   Overall Security Score: {sec['overall_score']:.1f}/100\")\n",
    "    print(f\"   Attack Resistance: {sec['attack_resistance']:.1f}/100\")\n",
    "    print(f\"   Key Strength: {sec['key_strength']} bits\")\n",
    "\n",
    "print(f\"\\nü§ñ BEST MODEL:\")\n",
    "if best_model:\n",
    "    print(f\"   Model: {best_model['name'].replace('_', ' ').title()}\")\n",
    "    print(f\"   Clear Accuracy: {best_model['clear_accuracy']:.4f}\")\n",
    "    print(f\"   FHE Accuracy: {best_model['fhe_accuracy']:.4f}\")\n",
    "    print(f\"   Accuracy Loss: {best_model['accuracy_loss']:.4f} ({best_model['accuracy_loss']/best_model['clear_accuracy']*100:.2f}%)\")\n",
    "    print(f\"   Composite Score: {best_model['composite_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° KEY RECOMMENDATIONS:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(f\"\\n‚≠ê FINAL ASSESSMENT:\")\n",
    "overall_grade = \"A\" if (best_config[\"performance\"][\"overall_score\"] >= 0.85 and \n",
    "                       best_config[\"security\"][\"overall_score\"] >= 85) else \\\n",
    "               \"B\" if (best_config[\"performance\"][\"overall_score\"] >= 0.75 and \n",
    "                       best_config[\"security\"][\"overall_score\"] >= 75) else \\\n",
    "               \"C\" if (best_config[\"performance\"][\"overall_score\"] >= 0.65) else \"D\"\n",
    "\n",
    "print(f\"   Overall Grade: {overall_grade}\")\n",
    "print(f\"   Readiness: {'Production Ready' if overall_grade in ['A', 'B'] else 'Needs Improvement'}\")\n",
    "print(f\"   Confidence: {'High' if overall_grade == 'A' else 'Medium' if overall_grade == 'B' else 'Low'}\")\n",
    "\n",
    "# Save best configuration to file\n",
    "config_output = {\n",
    "    \"best_configuration\": best_config,\n",
    "    \"best_model\": best_model,\n",
    "    \"overall_grade\": overall_grade,\n",
    "    \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "output_file = Path('../data/results/best_configuration.json')\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nüíæ Best configuration saved to: {output_file}\")\n",
    "print(f\"‚ú® Evaluation analysis completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
